{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":928025,"sourceType":"datasetVersion","datasetId":500970}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q torch torchaudio timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:48:35.186904Z","iopub.execute_input":"2025-05-24T05:48:35.187184Z","iopub.status.idle":"2025-05-24T05:48:38.345584Z","shell.execute_reply.started":"2025-05-24T05:48:35.187165Z","shell.execute_reply":"2025-05-24T05:48:38.344646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\nmodel = AutoModelForAudioClassification.from_pretrained(\n        \"MIT/ast-finetuned-audioset-10-10-0.4593\",\n        ignore_mismatched_sizes=True,\n        num_labels=10\n).cuda()\n\nfeat_extractor = AutoFeatureExtractor.from_pretrained(\n        \"MIT/ast-finetuned-audioset-10-10-0.4593\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:52:23.059570Z","iopub.execute_input":"2025-05-24T05:52:23.060244Z","iopub.status.idle":"2025-05-24T05:52:23.764662Z","shell.execute_reply.started":"2025-05-24T05:52:23.060220Z","shell.execute_reply":"2025-05-24T05:52:23.763915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchaudio, torch, pandas as pd, os\nROOT = \"/kaggle/input/urbansound8k\"\n\nmeta = pd.read_csv(f\"{ROOT}/UrbanSound8K.csv\")\nid2label = {i:c for i,c in enumerate(sorted(meta['class'].unique()))}\nlabel2id = {c:i for i,c in id2label.items()}\nmeta[\"label_id\"] = meta[\"class\"].map(label2id)\n\ndef load_resample(path):\n    wav, sr = torchaudio.load(path)\n    wav = wav.mean(0)          # mono\n    if sr != 16000:\n        wav = torchaudio.functional.resample(wav, sr, 16000)\n    return wav","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:52:30.066235Z","iopub.execute_input":"2025-05-24T05:52:30.066761Z","iopub.status.idle":"2025-05-24T05:52:30.094973Z","shell.execute_reply.started":"2025-05-24T05:52:30.066735Z","shell.execute_reply":"2025-05-24T05:52:30.094457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UrbanSoundHF(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n        self.target_len = 16000 * 10.24  # 163840 samples = 10.24 seconds\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        file_path = f\"{ROOT}/fold{row.fold}/{row.slice_file_name}\"\n        waveform, sr = torchaudio.load(file_path)\n        waveform = waveform.mean(0)  # mono\n\n        # Resample to 16kHz if needed\n        if sr != 16000:\n            waveform = torchaudio.functional.resample(waveform, sr, 16000)\n\n        # Pad or trim to target length (163840 samples)\n        if waveform.shape[0] < self.target_len:\n            pad_len = int(self.target_len - waveform.shape[0])\n            waveform = torch.nn.functional.pad(waveform, (0, pad_len))\n        else:\n            waveform = waveform[:int(self.target_len)]\n\n        return waveform, row.label_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:52:32.306603Z","iopub.execute_input":"2025-05-24T05:52:32.307158Z","iopub.status.idle":"2025-05-24T05:52:32.312888Z","shell.execute_reply.started":"2025-05-24T05:52:32.307137Z","shell.execute_reply":"2025-05-24T05:52:32.312052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = UrbanSoundHF(meta)\nfor i in range(5):\n    wav, label = dataset[i]\n    print(f\"Sample {i}: shape = {wav.shape}, label = {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:52:34.073989Z","iopub.execute_input":"2025-05-24T05:52:34.074256Z","iopub.status.idle":"2025-05-24T05:52:34.162137Z","shell.execute_reply.started":"2025-05-24T05:52:34.074238Z","shell.execute_reply":"2025-05-24T05:52:34.161415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate(batch):\n    wavs, labels = zip(*batch)\n\n    for i, w in enumerate(wavs):\n        if len(w) < 400:\n            print(f\"❗ Short waveform at index {i}: len = {len(w)}\")\n        if not isinstance(w, torch.Tensor):\n            print(f\"❗ Not a tensor at index {i}: type = {type(w)}\")\n\n    # 🛠 Convert to list of float32 numpy arrays for HF feature_extractor\n    wavs = [w.numpy().astype(\"float32\") for w in wavs]\n\n    inputs = feat_extractor(wavs, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    return {**inputs, \"labels\": torch.tensor(labels)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:57:57.019485Z","iopub.execute_input":"2025-05-24T05:57:57.020282Z","iopub.status.idle":"2025-05-24T05:57:57.026888Z","shell.execute_reply.started":"2025-05-24T05:57:57.020242Z","shell.execute_reply":"2025-05-24T05:57:57.025875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torch.nn.functional import cross_entropy\nfrom tqdm import tqdm\nfrom torch import nn\n\nall_acc = []\nfold_accuracies = [] \n\nfor fold in range(1, 11):\n    train_df = meta[meta.fold != fold]\n    val_df   = meta[meta.fold == fold]\n\n    train_dl = DataLoader(UrbanSoundHF(train_df), batch_size=4,\n                          shuffle=True, collate_fn=collate, num_workers=4)\n    val_dl   = DataLoader(UrbanSoundHF(val_df), batch_size=4,\n                          shuffle=False, collate_fn=collate, num_workers=4)\n    \n    # fresh classification head each fold\n    try:\n        in_features = model.classifier.dense.in_features  # original\n    except AttributeError:\n        in_features = model.classifier[0].in_features   # ASTMLPHead uses a \"dense\" layer internally\n    model.classifier = nn.Sequential(\n        nn.Linear(in_features, 512),\n        nn.Tanh(),\n        nn.Dropout(0.1),\n        nn.Linear(512, 10)\n    ).to(\"cuda\")\n\n    optim = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n    best = 0\n    for epoch in range(8):\n        model.train()\n        for batch in tqdm(train_dl, desc=f\"Fold {fold} Epoch {epoch}\", leave=False):\n            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n            outs = model(**batch)\n            loss = outs.loss\n            loss.backward(); optim.step(); optim.zero_grad()\n    \n        # Validation\n        model.eval(); correct = 0\n        with torch.no_grad():\n            for batch in val_dl:\n                batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n                logits = model(**batch).logits\n                preds = logits.argmax(1)\n                correct += (preds == batch[\"labels\"]).sum().item()\n        acc = correct / len(val_dl.dataset)\n        fold_accuracies.append(acc)\n        print(f\"Fold {fold} Epoch {epoch} Accuracy: {acc:.4f}\")\n\nall_acc.append(fold_accuracies)\nprint(f\"\\n10-fold mean accuracy: {sum(all_acc)/10:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T05:58:01.186814Z","iopub.execute_input":"2025-05-24T05:58:01.187502Z","execution_failed":"2025-05-24T06:11:17.280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nfor i, acc_list in enumerate(all_acc, 1):\n    plt.plot(acc_list, label=f\"Fold {i}\")\n\nplt.title(\"Validation Accuracy vs. Epochs (Per Fold)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/ast_urbansound8k_finetuned.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}